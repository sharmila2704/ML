{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmila2704/ML/blob/main/Paisa_Bazzar_CLEANED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **SmartScore: AI-Driven Credit Evaluation for Paisabazaar**\n"
      ],
      "metadata": {
        "id": "RK0LbKYas2Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Project Type    - EDA & Classification**\n"
      ],
      "metadata": {
        "id": "rIrD3qA2tk_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**\n",
        "The Paisabazaar Banking Fraud Analysis project is a supervised machine learning classification initiative focused on evaluating and predicting the creditworthiness of customers using their financial behavior data. Paisabazaar, being a financial services platform, needs to assess an individual's ability to repay loans by analyzing their credit scores. This score is crucial for making lending decisions, managing risks, and offering personalized financial products.\n",
        "\n",
        "The dataset includes key attributes such as income, credit card usage, outstanding debts, monthly investments, and payment behavior. The project starts with Exploratory Data Analysis (EDA) to understand data patterns, distributions, and relationships between features. Techniques like feature engineering (e.g., debt-to-salary ratio), data cleaning, and handling class imbalance using SMOTE are applied to improve data quality and model performance.\n",
        "\n",
        "Three machine learning modelsâ€”Random Forest, XGBoost, and Logistic Regressionâ€”are trained and compared. To further enhance accuracy, a Stacking Ensemble Classifier is implemented, combining the strengths of multiple models. The best-performing model is selected based on evaluation metrics like accuracy, precision, recall, and confusion matrix analysis.\n",
        "\n",
        "This credit score classification system can help Paisabazaar automate risk assessments, reduce default rates, and support smarter, data-driven lending decisions. The final model is also saved for future deployment, ensuring real-time prediction capabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "zoAb9rSruhdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "fHdIbEehvDrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement -**\n",
        "To implement a machine learning-based classification system that can accurately predict the credit score category (Poor, Standard, Good) of customers based on their financial behavior and demographic attributes.\n"
      ],
      "metadata": {
        "id": "gjaoq5IUvH1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Vu1DPH9Jsxkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "j_SS0OnqwY51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/MyDrive/dataset-2.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "F6oC9z_qs_OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***\n"
      ],
      "metadata": {
        "id": "KOIX8szhwVY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "6hVkDTf_pGTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gciTodEQpH7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#duplicate values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "MSFNXM0-pL_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "uQluaUR5pQuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "anXl1Wjfpbk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "1vU5nq2AwD6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils import resample\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "def clean_and_prepare_data(df):\n",
        "    df = df.drop(columns=[\"ID\", \"Customer_ID\", \"Name\", \"SSN\", \"Month\", \"Type_of_Loan\"])\n",
        "\n",
        "    # Fill missing values\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "    # Encode target variable\n",
        "    label_enc = LabelEncoder()\n",
        "    df['Credit_Score'] = label_enc.fit_transform(df['Credit_Score'])\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    cat_cols = ['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']\n",
        "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "    return df, label_enc\n",
        "\n",
        "def scale_and_split(df, target='Credit_Score', test_size=0.2):\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return train_test_split(X_scaled, y, test_size=test_size, random_state=42), X.columns\n",
        "\n",
        "def train_models(X_train, y_train, X_test, y_test, models_dict):\n",
        "    results = {}\n",
        "    trained_models = {}\n",
        "\n",
        "    for name, model in models_dict.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "\n",
        "        print(f\"\\nðŸ“Œ {name} Accuracy: {acc:.4f}\")\n",
        "        print(\"Classification Report:\\n\", classification_report(y_test, preds))\n",
        "\n",
        "        results[name] = acc\n",
        "        trained_models[name] = model\n",
        "\n",
        "    return results, trained_models\n",
        "\n",
        "def plot_feature_importance(model, feature_names, title=\"Feature Importances\"):\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[-10:]\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(np.array(feature_names)[indices], importances[indices])\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Importance Score\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"âš ï¸ Feature importances not available for this model.\")\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/MyDrive/dataset-2.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Clean and preprocess\n",
        "df, label_enc = clean_and_prepare_data(df)\n",
        "\n",
        "# Downsample for memory efficiency\n",
        "df_sampled = resample(df, n_samples=20000, random_state=42)\n",
        "\n",
        "# Split and scale data\n",
        "(X_train, X_test, y_train, y_test), feature_names = scale_and_split(df_sampled)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results, trained_models = train_models(X_train, y_train, X_test, y_test, models)\n",
        "\n",
        "# Select best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "best_model = trained_models[best_model_name]\n",
        "print(f\"\\nâœ… Best Model: {best_model_name} with Accuracy = {results[best_model_name]:.4f}\")\n",
        "\n",
        "# Feature Importance\n",
        "plot_feature_importance(best_model, feature_names, title=f\"Top 10 Features - {best_model_name}\")\n",
        "\n",
        "# Save the model\n",
        "with open(\"final_optimized_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "# Reload and sample prediction\n",
        "with open(\"final_optimized_model.pkl\", \"rb\") as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "sample_pred = loaded_model.predict(X_test[0].reshape(1, -1))\n",
        "print(\"\\nðŸ” Sample Prediction:\", label_enc.inverse_transform(sample_pred)[0])"
      ],
      "metadata": {
        "id": "5ToYGc5ubW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "a7tmJtzGv6ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x = df['Credit_Score'], hue = df['Credit_Score'], palette = 'viridis', order = df['Credit_Score'].value_counts().index)\n",
        "#Set labels and title\n",
        "plt.title('Distribution of Credit Scores')\n",
        "plt.xlabel('Credit Score Category')\n",
        "plt.ylabel('Count')\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2EPfFdVopmqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['Age'], bins = 30, kde = True, color = 'green')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GRRWv13rpvnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['Annual_Income'], bins = 30, kde = True, color = 'black')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Annual Income')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WMmQ14-iqBQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['Credit_Utilization_Ratio'], bins = 30, kde = True, color = 'orange')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Credit Utilization Ratio')\n",
        "plt.xlabel('Credit Utilization Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9daCiVO8rLSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Replicate necessary data loading and preprocessing steps from cell 5ToYGc5ubW4b\n",
        "# This ensures X_test and y_test have the correct number of features for the trained models\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/MyDrive/dataset-2.csv'\n",
        "df_confusion_matrix = pd.read_csv(file_path)\n",
        "\n",
        "# Clean and preprocess (using the function from cell 5ToYGc5ubW4b)\n",
        "def clean_and_prepare_data_for_cm(df):\n",
        "    df = df.drop(columns=[\"ID\", \"Customer_ID\", \"Name\", \"SSN\", \"Month\", \"Type_of_Loan\"])\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    label_enc_cm = LabelEncoder()\n",
        "    df['Credit_Score'] = label_enc_cm.fit_transform(df['Credit_Score'])\n",
        "    cat_cols = ['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']\n",
        "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "    return df, label_enc_cm\n",
        "\n",
        "df_cleaned_cm, label_enc_cm = clean_and_prepare_data_for_cm(df_confusion_matrix)\n",
        "\n",
        "\n",
        "# Downsample for memory efficiency\n",
        "df_sampled_cm = resample(df_cleaned_cm, n_samples=20000, random_state=42)\n",
        "\n",
        "\n",
        "# Split and scale data\n",
        "def scale_and_split_for_cm(df, target='Credit_Score', test_size=0.2):\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "    scaler_cm = StandardScaler()\n",
        "    X_scaled = scaler_cm.fit_transform(X)\n",
        "    return train_test_split(X_scaled, y, test_size=test_size, random_state=42), X.columns\n",
        "\n",
        "(X_train_cm, X_test, y_train_cm, y_test), feature_names_cm = scale_and_split_for_cm(df_sampled_cm)\n",
        "\n",
        "\n",
        "# Redefine and retrain models to get trained_models with models trained on the correct data split\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_cm, y_train_cm)\n",
        "    trained_models[name] = model\n",
        "\n",
        "\n",
        "# Confusion Matrix Heatmaps for each model\n",
        "fig, axes = plt.subplots(1, len(trained_models), figsize=(6 * len(trained_models), 5))\n",
        "axes = axes.flatten() if len(trained_models) > 1 else [axes]\n",
        "\n",
        "for idx, (name, model) in enumerate(trained_models.items()):\n",
        "    preds = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
        "    axes[idx].set_title(f\"Confusion Matrix - {name}\")\n",
        "    axes[idx].set_xlabel(\"Predicted\")\n",
        "    axes[idx].set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sl2FVW9VqsVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Bar Chart Comparison\n",
        "model_names = list(results.keys())\n",
        "accuracies = list(results.values())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=model_names, y=accuracies, palette='viridis')\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0.7, 1.0)  # Adjust based on your result range\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S1zxaitBdj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5.ML Models Used -***\n",
        "**1.Random Forest Classifier**\n",
        "\n",
        "An ensemble learning model that builds multiple decision trees and averages their predictions.\n",
        "\n",
        "Handles non-linear relationships and works well with imbalanced and noisy data.\n",
        "\n",
        "**2.XGBoost Classifier**\n",
        "\n",
        "An optimized gradient boosting algorithm known for speed and performance.\n",
        "\n",
        "Effectively captures complex patterns and handles overfitting through regularization.\n",
        "\n",
        "**3.Logistic Regression**\n",
        "\n",
        "A linear model used for multiclass classification.\n",
        "\n",
        "Interpretable and performs well when the relationship between features and output is mostly linear.\n",
        "\n",
        "**4.Stacking Classifier (Final Model)**\n",
        "\n",
        "Combines predictions from Random Forest, XGBoost, and Logistic Regression.\n",
        "\n",
        "Uses Logistic Regression as a meta-model to improve accuracy and reduce individual model bias."
      ],
      "metadata": {
        "id": "nveTA0MOx75w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***6.Evaluation Metrics Used -***\n",
        "**1.Accuracy** â€“ Measures the proportion of correctly predicted samples over total samples.\n",
        "\n",
        "**2.Precision** â€“ Measures how many of the positively predicted samples were actually positive.\n",
        "\n",
        "**3.Recall** â€“ Measures how many actual positive samples were correctly predicted.\n",
        "\n",
        "**4.F1 Score** â€“ Harmonic mean of precision and recall (used when class distribution is imbalanced).\n",
        "\n",
        "**5.Confusion Matrix** â€“ Gives a complete view of model performance across all classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "LuOkJj_FyqcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***7.Conclusion -***\n",
        "This project successfully built a machine learning-based system to classify customer credit scores using financial and behavioral data. With techniques like EDA, feature engineering, SMOTE, and model stacking, the final model achieved high accuracy and is ready for deployment. It enables Paisabazaar to improve loan decisions, reduce credit risk, and detect potential fraud more effectively.\n",
        "\n",
        "The Stacking Classifier achieved the highest accuracy (~91%) and balanced performance across all metrics, making it the best choice for deployment in predicting credit scores accurately.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_oLO8mjgwnaP"
      }
    }
  ]
}